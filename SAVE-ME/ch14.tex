\chapter{Parallel Execution with MapReduce}

\section{Vertical and Horizontal Scaling}

Some important computer applications are so large that they would take
unacceptably long to execute on typical computers.  For
example, your personal computer is more than powerful enough
to balance your checkbook, but not for a financial
application that tracks credit card usage in real time to
detect instances of fraud.  The sheer number of credit card
transactions make this application far too time-consuming for a
personal computer to handle.

To handle large applications of computers,
computer scientists have come up with different ways of coping
with problems of scale.  The traditional way is called
\emph{vertical scaling}, and it consists of running the
application on a single, very powerful machine.
This is, of course, the easiest possible solution in terms of software
because the software does not need to change as the machine scales up. 
But what happens as the problems get bigger than any single machine can handle?  
For example, maybe big, fast computer could handle 
a billion credit card transactions an hour,
but not a few hundred billion. 
At some point, the rate at which transactions take place
will overwhelm the available computing technology.

\emph{Horizontal scaling} offers an alternative.  Instead of
running the application in a single, large computer, 
the application is split into smaller
chunks, and each chunk is run on a separate computer.
Ideally, the computers used are similar in almost all respects
to personal computers, so that the cost of an additional
machine is a small fraction of the overall system cost.  
This makes it economically feasible to scale the
hardware platform as the computation requirement increase.  
Not surprisingly, horizontal scaling has become the de facto
solution for dealing with rapidly increasing scale web services, 
such as those provided by Facebook, Google, eBay, Amazon, Netflix, and many others.

The problem with horizontal scaling, however, is that it is
not always easy to split your application into smaller
chunks.  In fact, this is particularly hard when the program
is written using traditional programming languages, such as
C++ or Java.  In those languages, the program is described as
a sequence of steps that the computer must take.  But that
becomes more difficult to do as the number of computers
increases, because the programmer must also take into
consideration the interaction between the computers.
Getting the data and the software to the right computer
at the right time presents a host of problems.

One advantage of the equation-based software model
that has been a focus in this book is that the
different parts of the software are more decoupled 
than in the conventional software models presented
by programming languages like Java and C++.
Everything is defined in terms of operators
that, given the appropriate data through their operands
can produce their results without 
interacting with any other part of the software.
For that reason, it does not matter where or when 
the operations are performed.
The problems of getting the right data to the right place
remains, but the task of managing a great many small interactions
between different parts of the software is greatly reduced.

The engineers at Google were faced with one of the largest
scaling problems that computing had ever seen, 
namely searching the entire web.
To cope with the scale of this problem and the continual
increase in that scale,
the horizontal scaling approach was the only practical option.  
Even though they use conventional programming languages
(mostly C++, according to reports) for most of their software,
they invented and adopted a way to manage large-scale components
with a programming model called MapReduce, 
which has a lot in common the equation-based model of this book.  
Since Google's introduction of MapReduce, 
it has been adopted in many other settings.  For
instance, the Apache Foundation implemented Hadoop, an
open-source implementation of MapReduce that you can freely
download to your computer.  
Hadoop is widely used in industry and academia.  
We will plunge into all the details of the
MapReduce or Hadoop systems, we will try to
explain how the MapReduce framework simplifies the
development of programs that can scale horizontally by
focusing on just two operations, called, as you can
no doubt guess, Map and Reduce.

\section{Introduction to MapReduce}

The MapReduce paradigm is applicable to problems that process
data that can be represented as a sequence of key/value pairs.  
Not all problems lend themselves to this representation,
but Google engineers recognized that many practical
problems do fit in this category.  Here are some examples.

\begin{quote}
\begin{itemize}
\item \emph{Counting Words in a Document.}  The data in this
case is the collection of words in a document.  It can be
organized as a list of word/count pairs, where the counts
are initially set to one.  Note that each word may have more
than one word/count pair initially.  
The objective is to produce a list of word/count pairs 
in which each word appears only once.
\item \emph{Finding Words that Link to a Webpage.}  The
purpose of this operation is to find the words that are used
most commonly to link to a particular page.  For example,
your name may be the most common phrase used to link into your
Facebook page.  Google uses this kind of information to select which
pages to display for a particular search.  This problem, too,
can be solved using the MapReduce approach.  The data is the
collection of links on the Internet.\footnote{Nobody knows how many
links there are on the internet, worldwide, 
but most estimates place it in the trillions, 
and some credible estimates place it in the hundreds of trillions. } 
Each link can be represented as a word/URL pair, 
where the same word may appear in many different pairs.
To figure out which most commonly link to a particular page,
this data needs to be reduced to a collection of
URL/word pairs, where each URL will appear once, associated
with the word that is used the most to link to that URL.
\item \emph{Finding Extreme Values.}  Consider
an application that finds the record high and low temperature
for each state.  The initial data consists of a list of
city/temperature data.  There would be one record for each
recorded temperature in a city. It might be one per day, 
one per hour, one per minute, or a varying combination,
depending on the city, and the records would extend over
different periods of time for different cities, 
a hundred years for some cities, ten years for others,
and so on. So, each city would occur in many different pairs.
The desired outcome is a collection of city/temperature
pairs where each city occurs only once, and the associated
temperature is the highest (or lowest) one in the recorded data.
\end{itemize}
\end{quote}

What all these applications have in common is that the data can
be split into three different parts, each composed of a list of
key/value pairs, and it is possible to process each individual 
data record or key/value pair without having to simultaneously 
examine all the other records.  For
example, consider the case of finding extreme temperatures,
and in particular suppose that we want to find the highest 
temperature recorded for each state.
The data can be split into the following three parts.
\begin{enumerate}
    \item \textbf{Input Data} consists of pairs that list the 
        temperature at a given location on a given date. Each
        pair has a key that may correspond to an identifier
        for the specific sensor that recorded the temperature,
        and a value that consists of the city and state where
        the sensor was located, the date the sensor measured
        the temperature, and the actual temperature on that date.
        For example, the input data may contain the following
        records
        \begin{itemize}
            \item KLAR, Laramie, WY, 2009-05-13, 41
            \item KLAR, Laramie, WY, 2009-05-14, 47
            \item KOUN, Norman,  OK, 2009-05-13, 76
            \item KOUN, Norman,  OK, 2009-05-14, 70
        \end{itemize}
        The first column (for example, KLAR) is the ``key'' for each record,
        and the remaining columns are the ``value.'' Notice how 
        more than one record may have the same key in the input data,
        but the goal is to produce output where each key appears only
        once.
    \item \textbf{Intermediate Data} is used only to pass data from
        the Map operation to the Reduce operation. The Map operation
        processes the \emph{input data} and produces key/value pairs
        in the form of \emph{intermediate data}, and the Reduce operation
        processes the \emph{intermediate data} to create \emph{output data}.
        In our particular example, the goal is to find temperature extremes
        by state, so it makes sense that the Map operation extracts only
        the state and temperature from each input data record. So the
        intermediate records will look as follows
        \begin{itemize}
            \item WY, 41
            \item WY, 47
            \item OK, 76
            \item OK, 70
        \end{itemize}
        In general, the Map operation may produce any number of intermediate
        data points for any given input data record, although in this case
        precisely one intermediate record is generated for each input record.
    \item \textbf{Output Data} is the final result of the entire MapReduce computation,
        and it is produced by the Reduce operation. In our example, this
        corresponds to the maximum temperature recorded for each state, so we
        may see records such as
        \begin{itemize}
            \item WY, 115
            \item OK, 120
        \end{itemize}
        Notice that there is only one record associated with each key, since the
        record corresponds to the final answer for that key, for example, the record high
        temperature for that state.
\end{enumerate}
As you can see, the data records are largely independent of one another, so the computer 
can process the May 13, 2009 temperature entry for Norman, OK without considering the 
May 13, 2009 entry for Laramie, WY.  This enables horizontal scaling, because the
different entries can be processed in different machines.

However, this application also shows the need for combining
the entries for a specific key at a later time.  For example,
to find the high temperature record for Oklahoma, it will be
necessary to consider all the entries for Oklahoma at some
point.

The MapReduce paradigm conforms well with these kinds of problems.
Processing is divided into two parts.  
The first part is the \texttt{map} function, which receives each
of the input key/value pairs, one at a time, processes the
pair, and produces an arbitrary number of intermediate
key/value pairs.  These intermediate pairs use keys that may
or may not be completely different than the input keys.  
In the word counting example, the intermediate keys may be the same
as the input keys, namely the words that are being counted.
On the other hand, when looking for words that are used to
link to URLs, the input keys are the words, but the
intermediate keys are the URLs. The MapReduce framework leaves
this choice up to the software designer,
and that is one of the reasons why MapReduce is so widely applicable.

The second step is the \texttt{reduce} function, which
combines all the entries for each intermediate key and
produces zero or more output key/value pairs.  As before, the
output key/value pairs may use the same keys as the
intermediate or input key/value pairs, or they may use
entirely different keys.

For example, consider the problem of counting words in a
document.  We can assume that the document has already been
read, and that it has been broken up into key/value pairs
where each key is a word and the value is always one. These
key/value pairs make up the input data for this problem. For
instance, the Gettysburg address could be represented by a
list of key/value pairs using a dot notation ( \emph{key} ~.~ \emph{value} )
to denote a pair.
\begin{quote}
\begin{itemize}
\item ( four~.~1 )
\item ( score~.~1 )
\item ( and~.~1 )
\item ( seven~.~1 )
\item ( years~.~1 )
\item \dots
\item ( from~.~1 )
\item ( the~.~1 )
\item ( earth~.~1 )
\end{itemize}
\end{quote}

The map function takes in an input key and
value, and delivers a list of zero or more intermediate
key/value pairs.  For the word-count program, map could 
deliver a list with exactly one element, namely
the very same input key and value.
In this case, the map part of MapReduce 
packages the result in a list, which is
the form expected by the MapReduce system,
but does not perform any additional computation.
\begin{displaymath}
map(k, v) = [ ( k ~.~ v ) ]
\end{displaymath}
In a more elaborate example, map would perform
a computation using the key/value pair supplied as
its operand and deliver a list representing the results
of that computation.

The reduce function accepts an intermediate key and a list of
all the values returned by any map operation for that key.  
It delivers a list of zero or more final key/value pairs.  
In the case of word-count, reduce returns only one key/value pair, 
namely the key and the sum of the counts in the list.
\begin{displaymath}
reduce(k, vs) = [ ( k ~.~ sumlist(vs) ) ]
\end{displaymath}
The function sumlist, which adds all the elements of its
input, would be defined by the software designer.

To make this discussion more concrete, consider the Gettysburg Address,
which mentions the word ``nation'' precisely four times. Because of this,
the map function will be called with the input key/value pair \texttt{(nation~.~1)}
precisely four times, and each time it will return a list with
the single intermediate key/value pair \texttt{[(nation~.~1)]}. The MapReduce
system collects all the values for each intermediate key, and call the reduce
function on those values. So at some point it will collect all the four intermediate
values for ``nation'' and invoke the reduce function with the arguments
\texttt{nation} and {(1 1 1 1)}. Of course, the reduce function will then
return a list with the final key/value pair \texttt{[(nation~.~4)]}.

% \todo{QUESTION (11/23/17): I don't think this makes sense.
%      Where does vs come from? Earlier, map produced [(k~.~v)].
%      There was no list of vs, just one v. It seems that
%      reduce could create the vs list, but map couldn't supply it
%      a list of vs for each k, but not a list of all the v's 
%      associated with a particular k-value. The reduce step
%      could coalesce the duplicate k-values, but that's
%      it wouldn't help much to have reduce sum the v's for
%      non-coalesced k-values.
%      Maybe it would be better to define
%      map(k, v) = v, then explain that the MapReduce
%      system forms a list of all the results produced
%      by map when it operates on the key/value pairs, one by one,
%      then talk about what the reduce step does in this example.
%      Or something.}

Much of the value of MapReduce is that the programmer only needs to
define the map and reduce functions as above, and the programmer
does not need to deal with the details of executing the map and reduce
functions in a network of computers or sharing the data across the
network.  It is the MapReduce
framework that takes care of running the program in a single
computer, or in a cluster of hundreds or even hundreds of thousands 
of computers, depending on the size of the problem. And the MapReduce
framework takes care of sending the intermediate key/value pairs
from the computers processing the map function to the computers
processing the appropriate reduce function.

What does the MapReduce framework do?  First, it
takes the input key/value pairs and splits them across many
different machines.  On each machine, it performs the map
operation on each of the key/value pairs that is assigned to
that machine.  As it does this, it combines the intermediate
key/value pairs returned by each map operation into a single
list.  The lists from all of the machines are then combined.

It may seem unnecessary to combine all of the intermediate lists
before calling reduce on a single intermediate key.  
However, those intermediate lists do need to be combined because 
the reduce function needs to see an intermediate key and all 
of the values associated with that key at the same time. For example,
to find the maximum temperature for the key ``OK'', the reduce
task needs to see all the records associated with ``OK'' at once.
Different intermediate keys, such as the ones for ``OK'' and ``WY'',
are independent, so they can be processed by different machines.
That is, one machine can process ``OK'' temperatures at the same
time that another machine is processing ``WY''.
But all of the ``OK'' records must go to the same machine running
the reduce function for ``OK''.
Therefore, the map function collects all of the values for
each of the intermediate keys, and then the reduce operation is
called only once for each intermediate key.

Once all the values for a given intermediate key are
collected, the MapReduce framework can call the reduce
function on that intermediate key.  The result is a list of
final key/value pairs, and MapReduce collects all these
results and returns them as the final answer.

As you can see, MapReduce is doing all of the work
related to distributing the program across multiple machines,
as it was designed to do.
That is one way it provides value. 
An engineer can develop a MapReduce program on
a local computer, modify it until it behaves as
required on a small data set.  
Then, the program can be submitted to a large MapReduce cluster
to process an entire data set, which may be very large.
The MapReduce system deals with the problem of scale
automatically, once the program has been developed.

\section{Data Mining with MapReduce}

Now that we have seen the basics of MapReduce, we can look at
a larger example, something that illustrates how MapReduce is
used in practice.  The application is a recommendation engine, 
a piece of code that is used to recommend new things to a person
on other things the person likes.  
For example the product page that pops up on a typical visit to
the Amazon website often has a section called ``Customers Who Bought
This Item Also Bought'' that recommends related items.  
Based on past purchases and browsing habits, the Amazon software 
builds a customized web page full of recommendations.
How does Amazon do this?

The answer is easy to understand after we break the problem down into
two components.  First, Amazon needs to finds \textit{customers like
you}.  In the case of a single product, this means other customers who
have bought this product.  In the more general sense, used to create custom
recommendation lists for you, it means other customers who have bought many
of the same items that you have purchased in the past.  Once the group of
customers like you is identified, the rest of the problem is easy.  Each
person in that group has made some purchases, so it is only necessary to
find the most popular items in that group.

Finding the most popular items is essentially the same as counting words
in a document.
Amazon keeps a history of all the purchases that each of its
customers have made.  To process this list with MapReduce, think of it as
consisting of entries of the form customer/item, meaning that the given
customer bought that particular item.  Similar to the word-count program,
the map operation produces intermediate entries of the type item/1, meaning
the given item was bought (once).  Such an entry should be generated for
each purchased \textit{made by a customer in the reference group.}  That is,
the map operation filters out the purchases made by customers who are
not like you.  The reduce operation is identical to that of word-count, 
but this time it counts the number of purchases for each item.  
The remaining detail is to consider the results of the reduce operation 
and select the items that were purchased most often.

Unfortunately, that leaves the first problem unsolved: finding the group of
customers who are most like you.  This is the most critical aspect for
generating useful recommendations.  For instance, if all of you purchases
from Amazon have been gardening books, you are likely to ignore a
recommendation engine that alerts you to the latest novel in a long-running 
vampire series.  Worse, you may start thinking of the recommendations
as unwarranted spam.

How can Amazon find customers just like you?  Imagine, first of all, that
you have rated all the purchases you have made, giving each item a grade between
0 (hated it) and 5 (loved it).  To keep things simple, imagine that Amazon
sells only two items.  Then your ratings for these items can be expressed as
a pair of numbers, say (2, 0).  Now suppose that other customers have similarly
rated the items.  The customers who are most like you are precisely the ones
whose ratings are close to your score or (2, 0).  Effectively, your purchase
history is represented by a point at coordinate (2, 0), and the customers who
are most like you have purchase histories that correspond to nearby points.

Of course, Amazon sells many more than two items.  And neither you nor any
of Amazon's other customers are likely to have rated even a fraction of them.
But the principle stays the same.  Instead of using pairs to represent your
ratings, we need many more coordinates, as many as the number of 
different products Amazon sells.
But this is still just a point, albeit in a space with many dimensions.  
The customers most like you will still be represented by points close to yours.

A remaining complication is that customers do not always explicitly rate the
items they like, but this can be easily resolved by using implicit ratings.
For example, if you buy an item, we can give it a rating of 4, unless you
explicitly change it.  And any product that you have never even looked at
can be given a rating of 0.

So the problem is to find which points in this huge domain with many dimensions 
are near your own.  It is actually more useful to think of this a little differently.
Instead of finding points near yours, think in terms of finding groups of
points that are clustered together.  One cluster, for example, may consist of
avid gardeners, while another includes fans of vampiric fiction.

In general, finding clusters in a large data set is a very complicated problem,
but there are some useful approximations.  For example, suppose that you expect
to find ten clusters.  You could try to find them as follows:
\begin{quote}
\begin{enumerate}
    \item Initially, guess at the location of each cluster.  The guess can consist
        of the center point of each alleged cluster.
    \item For each point in the data set, decide which cluster center is nearest
        to the point.  Split the points into clusters so that each point is
        in the cluster determined by the nearest center point.
    \item Next, recalculate each cluster's center point by averaging all the
        points that were assigned to that cluster.
    \item Repeat the previous two steps as many times as necessary to find the
        clusters.
\end{enumerate}
\end{quote}
The middle two steps can be implemented using MapReduce.  The map operation can
assign points to clusters, while the reduce function can computes the new center
point of each cluster.  Note that if we know the center points, it is a trivial
matter to determine which cluster any other point belongs to.  Once we know that, 
we can determine, as we did before, which products are relevant to customers 
in your cluster.

We have to be a little careful with the definition of the map and reduce functions.
As in the earlier examples, the map function only has two arguments, a key and an
associated value.  In this case, the map functions needs a purchase history 
(that is, a point that represents an individual customer) and the current cluster center
points.  These center points should be the same for all purchase histories, so
it does not seem necessary that they should be passed to all invocations of the
map function.  However, this is the way that the MapReduce framework is defined,
so we have to pass these center points as one of the arguments to the map function, 
and the only one that makes sense is as the value. So the map function can be
defined like this
\begin{displaymath}
map(hist, centers) = [ ( closest\_center(hist, centers) ~.~ hist ) ]
\end{displaymath}
As you can see, the result of a map operation is an intermediate key/value pair, 
where the key identifies one of the cluster center points (the one closest to the
current point), and the value is the point representing the current history.

% \todo{QUESTION (11/23/17): But what is the purpose of v? 
%       The result does not seem to depend on it. Is this some kind of
%       symmetry thing, like the symmetry for reduce, below?
%       Needs some kind of explanation.}

The MapReduce framework will collect all the intermediate values and call the reduce
operation once for each key.  So, the reduce operation sees all the points in each
cluster, and it can compute the new center point of the cluster by averaging the points
in the list.  Although reduce does not need to see the original center points, we pass
these to the reduce function simply because that is the way the MapReduce framework is
defined. The keys for the reduce function are precisely the intermediate keys produced
by the map function. In terms of the clustering process, what the keys really do is
identify a particular cluster, such as ``cluster \#2'', but the way we identify a particular
cluster is by naming the point at the center of the cluster, such as ``the center of
cluster \#2''.

% \todo{QUESTION (11/23/17): Why is the symmetry necessary? 
%       Is that a requirement of the MapReduce system?
%       If so, we should explain that, or if we don't explain it, just ignore it an leave out
%       the symmetry, since it's confusing to include an unused operand. If MapReduce doesn't 
%       require symmetry, I find it confusing to insist on it in our treatment.}

% \todo{QUESTION (11/23/17): The last line said count+0.
%       I presume this should be count+1, so I changed it,
%       and I changed the name of the tail-recursive helper function to avg,
%       because of a bias against names with underscores, especially when
%       what comes after is aux or help or something like that.}

\begin{eqnarray*}
    \begin{array}{l@{}l}
        reduce(&cluster, \\
               &centers)
    \end{array} &=& [ ( cluster ~.~ average(points) ) ] \\
average(points) &=& avg(points, (0,0), 0) \\
\begin{array}{l@{}l}
    avg(&points, \\
        &sum, \\
        &count)
\end{array} &=&
    \left\{
        \begin{array}{l@{}ll}
            \multicolumn{2}{l}{sum / count} & \mbox{if } points = [~] \\
            avg(& rest(points),             & \mbox{otherwise} \\
                & sum+first(points),        & \\
                & count+1)                  & \\
        \end{array}
    \right.
\end{eqnarray*}

For completeness, we should explain some of the operators and terms
that these definitions refer to because they are not, in all cases,
what might be expected.
The \emph{points} variable refers to a list of points,
and each point is a pair of numbers. The first number in a point
denotes a product that Amazon sells and the second number is your rating
of the product.
The operand $(0,0)$ in the equation that defines $average$ is, like a point, 
made up of two numbers, both zeros, serving as a starting point for adding
up the product numbers and ratings to make it possible to compute the average.
The $sum$ variable is also like a point, consisting of a pair of numbers.
Therefore, the addition ($+$) and division ($/$) operators
with $sum$ as a left-hand operand are not the usual arithmetic operators.
The formula $sum + point$ denotes $(sk, sv) + (k,v)$, which stand for
the pair $(sk+k, sv+v)$. Similarly, $sum/count$ denotes $(sk,sv)/count$,
which stands for $(sk/count, sv/count)$. 
The definitions also refer to the operators first and rest.
The operator first delivers the first point in the collection of points,
and the operator rest delivers all the points in the collection
other than the first one. 
Finally, the equation defining $avg$ selects one of two formulas,
depending on the value of $points$, which is a list.
If the list is empty (that is, if $points = [~]$), 
the formula $sum/count$ is selected as the value of $avg$.
If $points$ is not the empty list, the definition selects
a more complicated formula as the value of $avg$.

There is an important subtlety in the way the equations define
the function $average$, which is used to compute the new center point of the cluster
from the points in the data set.
The work of computing the average is done in another function, $avg$.
The definition of $avg$ employs a common trick known as tail recursion
that makes computations like averaging much faster.
We think it is worth complicating the presentation 
with this trick because the whole point
of MapReduce is to be able to compute things quickly.
The tail recursion trick avoids nesting the value that $avg$ delivers
as an operand supplied to another function.
This makes the computation
faster because the computer is able to avoid a lot of bookkeeping that nested
invocations require.\footnote{An 
explanation of how tail recursion accomplishes this feat appears
in Section \ref{sec:lemmas} in the discussion of the fib-fast operator.}

% \todo{NOTE (11/24/17) I didn't get past this point}

The result of the reduce operation is the new list of center points.  
This should be in the same format as the initial guess.  
What this means is that the output of the
reduce operation can be passed back as 
the initial guess for subsequent passes of the map step.  
This allows us the computer to perform as many map and reduce operations as necessary
to find the clusters, as illustrated in Figure~\ref{iterative-map-reduce}.

\begin{figure}
    \begin{center}
        \includegraphics[scale=0.7]{images/iterative-map-reduce}
    \end{center}
    \caption{Iterative Map Reduce Operation}
    \label{iterative-map-reduce}
\end{figure}

\section{Summary}

Some problems are too large to solve on a single, conventional machine.  That leaves us
with two options.  We can either get a large machine (a supercomputer, for example), 
or we can break the problem down into many tasks and execute each task on a separate computer.
The first approach, vertical scaling, is limited by the size of the largest computer that
we can afford.  It is also difficult to use in practice, because as the problem grows,
several scaling steps may be
necessary, and each step requires an expensive migration to a more capable computer.
If we were trying to use vertical scaling to deal with the massive, rapid increases
in the volume of traffic on a website like those of Google and Amazon,
that might mean an upgrade every few days, which would be impossible with
vertical scaling.

The alternative approach, horizontal scaling, offers the promise of virtually unlimited
scalability and incremental increase in the cost of the solution as the problem size grows.
But it is much more difficult to write programs that are split across multiple machines.
Also, this solution only applies to problems that can be reasonably split.  Such problems
usually involve a lot of data, and the data needs to be in clumps 
that can be processed independently.

MapReduce is a framework related to the equation-based software model that facilitates writing
programs that can be distributed across a large number of computers.  
MapReduce programs are organized around two functions.  
The Map function applies to each of the input values, and
it generates an intermediate result.  
The intermediate results are subsequently combined using the Reduce function.  
If the results generated by the Reduce function are in the same format
as the input to the Map function, multiple MapReduce passes can be performed.  
This is useful in programs that estimate optimal results by refining an initial estimate, 
such as programs that find clusters in a large data set.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
