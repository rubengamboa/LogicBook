\section{Limits of Boolean Equations}

There is an aspect of Boolean equations that we haven't fully considered. A Boolean variable $x$ stands for a proposition, 
such as ``Socrates is a man.'' What we mean by this is that the variable $x$ is $True$ precisely when ``Socrates is a man'' 
is true in the real world. We will soon move on to other ways of representing real-world knowledge using logic, but before 
we do that, we want to address the question, ``Just how powerful are Boolean equations by themselves?'' The answer, it 
turns out, is ``very powerful, indeed!''

But first, let's take an apparent detour into the ancient game of \emph{Nim}. There are many variants of this game, and we 
will consider a simple one. The game starts with a pile of ten stones, and two players, called Alice and Bob, take turns 
removing one, two, or three stones from the pile. The player who picks up the last stone loses. Here is a specific game 
that Alice won:

\begin{flushleft}
\begin{tabular}{l|l|l|l}
Move & Alice     & Bob      & Stones \\
\hline
0    &           &          & 10     \\
1    & Remove 2  &          & 8      \\
2    &           & Remove 3 & 5      \\
3    & Remove 1  &          & 4      \\
4    &           & Remove 2 & 2      \\
5    & Remove 1  &          & 1      \\
6    &           & Remove 1 & 0      \\
\end{tabular}
\end{flushleft}

Can we play the game of Nim using propositional logic? Let's try! We will certainly need Boolean variables to represent the 
pile of stones. For example, let $x$ be a variable that means ``there are 10 stones in the pile'' and $y$ be a variable that 
means ``there are 9 stones in the pile.'' This works, surely, but we're sure to run out of letters if we start with a bigger
pile. Instead, let $x10$ be the variable that means ``there are 10 stone in the pile,'' $x9$ that ``there are 9 stones in 
the pile,'' up to $x0$ for an empty board. The collection of these 11 Boolean variables can describe any pile.

Actually, they can describe piles that are impossible, so we need rules to limit the possible values of these variables. 
For example, a pile must have some number of stones between zero and ten. We can capture this down using the rule
$$(x0 \vee x1 \vee x2 \vee \cdots \vee x10).$$
Also, no pile can have both zero and nine stones at the same time. We can write this down using the rule
$$\neg(x0 \wedge x1) \wedge \neg(x0 \wedge x2) \wedge \cdots \wedge \neg(x0 \wedge x10).$$
Of course, we also need similar rules for other possible configurations of stones, such as
$$\neg(x1 \wedge x2) \wedge \neg(x1 \wedge x3) \wedge \cdots \wedge \neg(x1 \wedge x10).$$
Do you see how this captures the idea that the pile cannot have both one and three stones? Do you also see
why this last rule does not have an explicit case for $\neg(x1 \wedge x0)$?

We're making progress, but we've fallen into a trap! It is not enough to describe the pile at a particular moment. 
We must also be able to describe how the number of stones in the pile changes over time as the players remove them.
So let's rethink the Boolean variables and add a time component to them. We now write $x_{10}^{3}$ to mean 
``there are 10 stones in the pile after 3 moves.'' For example, the situation after Bob removes three stones in the
sample game above would be described by $x_{5}^{2}$. 

It is important to realize that a Boolean variable like $x_{10}^{3}$
is still just a Boolean variable like $y$ or $z$. The superscript 10 and subscript 3 are just part of the name. It
is tempting to believe that since there is a variable called $x_{10}^{3}$, there must be many other variables with
names like $x_{12}^{7}$ and $x_{3}^{26}$. In fact, we could have used more conventional names, perhaps $abc$ instead
of $x_{10}^{3}$, and $bpe$ instead of $x_{5}^{2}$. But we like the superscript and subscript because it tells us at 
a glance how many stones are in the pile and how many moves have already elapsed. To keep things straight, remember 
that there are only 11 times 11, or 121, such variables, because the pile can only have between zero and ten stones, so there are
precisely 11 possible different piles, and there can be only 11 moves in any game (including the starting configuration 
of the pile), since each move removes at least one stone from the pile.

You can now see how we can write down the ``initial condition'' that there are ten stones in the pile at the beginning
of the game: $x_{10}^{0}$.
You should also be able to see that we need 10 additional formulas describing the possible piles 
after the players' moves. Here is one example for the pile after six moves:
$$(x_{0}^{6} \vee x_{1}^{6} \vee x_{2}^{6} \vee \cdots \vee x_{10}^{6}).$$
All of these 11 formulas should be combined using AND so we can describe all the possible legal piles of stones.

We also have to update the formulas that disallow a pile having both zero and five stones, for example. So we
need some rules like the following after six moves:
$$\neg(x_{0}^{6} \wedge x_{1}^{6}) \wedge \neg(x_{0}^{6} \wedge x_{2}^{6}) \wedge \cdots \wedge \neg(x_{0}^{6} \wedge x_{10}^{6}).$$
This formula disallows piles having both zero and five stones or zero and seven stones, but as before we need nine additional
formulas to disallow piles with both one and five stones or one and seven stones, such as the following:
$$\neg(x_{1}^{6} \wedge x_{2}^{6}) \wedge \neg(x_{1}^{6} \wedge x_{3}^{6}) \wedge \cdots \wedge \neg(x_{1}^{6} \wedge x_{10}^{6}).$$
That is a total of ten formulas that must be combined using AND to rule out impossible piles after step six. Of course,
we need a group of rules like this to describe the pile after zero moves, one move, two moves, and so on, up to ten moves. These
11 groups of rules should also be combined using AND, making 11 times 10, or 110, separate rules that are ANDed together.

What about the rules for removing stones from a pile? For example, if the pile has five stones after step three, then it must
have four, three, or two stones after four, since the player removes either one, two, or three stones in step four. This can
be captured with the following rule:
$$x_{5}^{3} \rightarrow (x_{4}^{4} \vee x_{3}^{4} \vee x_{2}^{4}).$$
We need only be careful when we get down to piles with fewer than three stones. For example, for the case when there are only 
two stones left in the pile, we need a rule like the following:
$$x_{2}^{3} \rightarrow (x_{1}^{4} \vee x_{0}^{4}).$$
And what do we do when we run out out of stones? To make the description as simple as possible, it is best to continue the pattern
and just ensure that there are no stones at the next turn:
$$x_{0}^{3} \rightarrow x_{0}^{4}.$$
Of course, many such rules will be needed, one rule for each possible combination of stones left and number of steps taken.

Finally, how do we know if Alice wins? According to the rules, Alice wins if Bob picks up the last stone, and since Alice
goes first, Bob must pick up the last stone at an even number of steps. In other words, we can capture Alice's victory
in the following Boolean formula:
$$x_{0}^{2} \vee x_{0}^{4} \vee x_{0}^{6} \vee x_{0}^{8} \vee x_{0}^{10}.$$

Now consider a single formula combining
\begin{enumerate}
\item the initial pile containing ten stones, AND
\item the possible legal descriptions of piles at all times, AND
\item the legal ways in which a pile transform from one step to the next, AND
\item the formula that ``Alice wins.''
\end{enumerate}
Do you see that if this formula is equal to $True$, then Alice must always win the game? And what if the formula is 
equal to $False$? In that case, do you see that Alice can never win the game? But do you see a third possibility? 
What if the formula is neither equal to $True$ nor equal to $False$? Is this even possible?

In fact, it is possible that the formula is neither $True$ nor $False$. What this means is that for some values of the 
Boolean variables the formula is $True$, but that for others the formula is $False$. Each possible combination of the 
Boolean variables corresponds to a single game of Nim, for example, one in which the Alice removed two stones, then Bob removed
three stones, then Alice removed one more stone, and so on. The value of the formula tells us whether Alice won that 
particular game. So if the value is True, you should be able to inspect the values of the individual Boolean variables 
and reconstruct a game where Alice won.

Now hang on to your hats, because we are about to introduce you to one of the central mysteries of computer science: 
What we just did for the game of Nim, we can also do for any computer program that \emph{finishes in a bounded number of steps}. 
What we need to do is to construct a formula that combines
\begin{enumerate}
\item the initial configuration of the computer, AND
\item the possible legal configurations of the computer at all times, AND
\item the legal transitions of the computer from one step to the next, AND
\item the formula that ``the program is finished.''
\end{enumerate}
Since digital computers store everything as a sequence of ones and zeros, the initial configuration is simply the initial 
values of those bits, either ones or zeros. The legal configurations rule out the possibility that a given bit is both a 
one and a zero at the same time. The legal transitions correspond to the actual program and how it manipulates information 
over time. And we can tell when the program is finished in many ways, for example by insisting that the last thing it does 
is set a specific bit to one.

So there you have it. If you know that a given program will execute in 10 million or fewer steps, then you can (in principle) 
write down a Boolean formula that completely captures its execution. In this sense, Boolean formulas are powerful enough to 
represent any computer program, as long as you can state ahead of time how long it will take to execute. So never give in to 
the fallacy that Boolean formulas are somehow limited.

\begin{ExerciseList}
\Exercise
Did you notice the mistake in the way we described the game of Nim? Suppose that Alice wins in step 5, so that $x_{0}^{6}$ is
true. According to our rules, that means that $x_{0}^{7}$ must also be true, but then from the description of ``Bob wins,''
you would conclude that both Alice and Bob won. That is not at all what we meant! Modify the formula that says ``Alice wins''
to take care of this complication.

\Exercise 
There are many variants of the game of Nim. In another variant, there are three piles of stones, and each player can remove
one or more stones from any single pile. The player who removes the last stone loses the game. Describe this game using
propositional reasoning.

\end{ExerciseList}

\section{Beyond Boolean Equations: Predicates and Quantifiers}

You now know that Boolean equations can be used to describe very complicated scenarios. They really are more powerful 
than may appear at first. But you have also seen how incredibly cumbersome it can be. It is time to learn about predicates, 
a simple extension to Boolean variables that simplifies the task of representing the real world in logic.

A predicate has a name and one or more arguments, for example, $man(socrates)$. 
The predicate is $man(X)$ and the $X$ is an argument that can be filled by any constant, 
such as $socrates$. A predicate is either $True$ or $False$, so for example, $man(socrates)$
precisely because Socrates is a man. In general, any property that is $True$ or $False$
depending on the value of its arguments is a predicate. We usually write predicates as
$man(socrates)$, but other predicate expressions are very established and have their 
own syntax, for example $5 < 10$ is a predicate that evaluates to $True$.
In other words, a predicate replaces a Boolean variable 
such as $x125$ with $x(1,2,5)$. This does not appear to be a major improvement, but the real power of predicates comes from 
the use of logical variables.

Logical variables stand for the objects under discussion. For example, if we are using logic to describe some aspect of
people, then a logical variable $X$ can stand for any one person. Above, we saw the predicate $man(socrates)$, which is $True$
precisely when the constant $socrates$ refers to a man. Using logical variables, we would say that $man(X)$ is $True$ precisely
when $X$ refers to a man. That is, if $X=socrates$, then $man(X)$ is $True$, and if $X=plato$ $man(X)$ is also $True$, but if
$X=hypatia$, then $man(X)$ is $False$. So the value of $man(X)$ depends on precisely which person $X$ stands for; it will be
$True$ for some, but $False$ for others.

This helps explain the power of predicates. Recall the use of Boolean variables in modeling the game of Nim, such as $x_{3}^{6}$
with the meaning ``There are three stones after six moves of the game.'' To reason about the situation when there are three
stones left, we have to consider 11 different Boolean variables, namely $x_{3}^{0}$, $x_{3}^{1}$, $x_{3}^{2}$, all the way 
up to $x_{3}^{10}$. But with predicates, we can instead refer to $X(3, t)$ where $t$ is a logical variable with the possible
values 0, 1, \dots, 10. Using this notation, we can express ourselves much more briefly.

But what exactly is the meaning of $X(3, t)$? It clearly depends on the value of $t$, so in a sense this expression by
itself is neither $True$ nor $False$, but it ``just depends.'' This is where quantifiers come in. There are two common
quantifiers that we can use to give a precise meaning to $X(3, t)$. The ``for all'' quantifier is written the symbol
$\forall$, which looks like an upside-down A. The expression $(\forall t.(X(3, t)))$ is $True$ precisely when $X(3, t)$
is $True$ for all possible values of $t$. 
For example, you may remember that we specified the fact that the pile after six steps
cannot have both zero and one stones, or zero and two stones, etc.:
$$\neg(x_{0}^{6} \wedge x_{1}^{6}) \wedge \neg(x_{0}^{6} \wedge x_{2}^{6}) \wedge \cdots \wedge \neg(x_{0}^{6} \wedge x_{10}^{6}).$$
Using quantifiers, we can write this more succinctly as follows:
$$(\forall s.(\neg(X(0, 6) \wedge s\ne0 \wedge X(s, 6)))).$$
In fact, we can do better than this. Using propositions, we needed to consider ten
different cases, one for each possible number of stones other than zero. With the 
``for all'' quantifier above, we need only one expression to describe all of those 
non-zero possibilities. But with propositions, we also needed to describe the fact
that no pile can have both one and two stones at the same time, or one and three
stones, etc.  All of those expressions had to be combined using AND. But using
logical variables for both possible counts, we can cover all possible combinations 
with a single expression:
$$(\forall s_1.(\forall s_2.(\neg(X(s_1, 6) \wedge s_1 \ne s_2 \wedge X(s_2, 6))))).$$
This is a clear improvement, but with logical variables we can do even better. As it
is currently written, the formula describes the situation only after six game moves,
so to describe the entire game we need ten more similar expressions and combine all
of them with AND. Using logical variables, we just need to add one more quantifier:
$$(\forall s_1.(\forall s_2.(\forall t.(\neg(X(s_1, t) \wedge s_1 \ne s_2 \wedge X(s_2, t)))))).$$
This single predicate expression is equivalent to over 500 expressions that were
necessary when using just propositions!

The other common quantifier is ``there exists'', which is written using the symbol $\exists$, 
which looks very much like a backwards E. The expression $(\exists t.(X(1, 2, t)))$ 
is $True$ if there is some possible value of t such that $X(1, 2, t)$ is $True$ 
for that value of $t$. For example, $(\exists n.(even(n)))$ is $True$ because there 
is a value of $n$, say $n=2$, such that $even(n)$ is $True$. On the other hand, 
$(\forall n.(even(n)))$ is $False$, because not all integers are even.

In the game of Nim, the propositional formula that the pile must contain between zero and
ten stones after six moves was written like this:
$$(x_{0}^{6} \vee x_{1}^{6} \vee x_{2}^{6} \vee \cdots \vee x_{10}^{6}).$$
Using ``there exists'', we can write that more succinctly as follows:
$$(\exists s.(X(s, 6))).$$
As before, we can use ``for all'' quantifiers to extend this requirement to all possible
points in time:
$$(\forall t.(\exists s.(X(s, t)))).$$
Notice, how the order of the quantifiers matters. This expression says that at any given
time, there is a specific number of stones in the pile. If the quantifiers were reversed,
the statement would indicate that there is a specific number such that the pile has that
many stones at all possible points in time---which is clearly false, since stones are
removed from the pile at each step in the game.

\begin{aside}
``For all'' and ``there exists'' are not the only possible logical quantifiers. For example, you may imagine a context
in which ``for most'' is a perfectly sensible quantifier. In some branches of mathematics, it is common to use the
quantifiers ``almost everywhere'' and ``almost nowhere'' which have precise mathematical definitions, although they
appear to be somewhat fuzzy in natural language.
\end{aside}

The true power of predicates comes from the use of quantifiers. For example, we saw in the 
previous section that it
is possible to characterize the game of Nim using just Boolean variables, with a formula that looks like this:
\begin{enumerate}
\item the initial pile containing ten stones, AND
\item the possible legal descriptions of piles at all times, AND
\item the legal ways in which a pile transform from one step to the next, AND
\item the formula that ``Alice wins.''
\end{enumerate}
In this section, we have seen how these formulas can be simplified greatly using predicate logic. In fact, with predicate
logic we can do more than just simplify the formulas---we can express more powerful ideas. Consider the formula with
meaning ``Alice wins.'' Previously, this was built by taking together the more concrete formulas ``Alice wins at
time 2,'' ``Alice wins at time 4,'' \dots, ``Alice wins at time 10'' and joining them all with ORs. Using predicates
and quantifiers, however, it is simpler to say $(\exists t.(AliceWinsAtTime(t)))$.

Resist the temptation to think that this is just a simple syntactic change. We have gained much more than the ability to
collect all possible ``Alice wins at time \dots'' phrases into a single statement. The reason is that there is that
the variable $t$ can potentially stand not just for the numbers zero through ten, but for any possible value of time. For example,
we saw that this same approach applies to more open-ended games like chess. Using propositions, we could (in principle)
completely describe the game of chess, but only if we limit such games to, say, 200 moves. With predicates, we have no such
limit---we can simply refer to an arbitrary (and vague) time at which black wins, for example.

To really drive this point home, remember that propositions can also be used to completely describe the execution of any
program, as long as you could place some limit on the total number of steps required for the computation. For instance, if
you could guarantee that the program would execute less than 1,000,000 steps, then you could (again, in principle) construct
a Boolean expression that was $True$ if and only if the program produced a given answer. With predicates, we can do much better.
We can actually describe an open-ended computation, for example, with ``there exists a time $t$ at which the program is finished.'' And
we can even use a logical variable to describe the answer, as in ``there exists a time $t$ and a value $a$ such that the program 
finishes at time $t$ with result $a$.'' In other words, predicate logic is powerful enough to describe \emph{all} possible
computer programs.

Now that you know about the expressive power of predicates and quantifiers, you're probably wondering about how we
can use these in proofs, and also how we can prove formulas that involve them. Let's tackle the first question. Suppose
we have already proved that $(\forall x.(P(5, x)))$, where $P$ is an arbitrary predicate. How can we use this predicate
in a subsequent proof? The answer is that $(\forall x.(P(5, x)))$ is essentially a short-hand for $P(5, 0)$
and $P(5, 1)$ and $P(5, 2)$ and \dots, for all possible values of $x$. So we can use $(\forall x.(P(5, x)))$ to rewrite
$P(5, 0)$ to $True$, or $P(5, 1)$ to $True$, or $P(5, 2)$ to $True$, or \dots. That is, once we have proved a ``for all'' 
formula, we can use that to justify a more specific theorem where the variable in the ``for all'' is replaced by any
expression that we find convenient. That is the meaning of ``for all.''

What about $(\exists x.(P(5, x)))$? Suppose we already proved this---how can we use it in a future proof. Because
the ``there exists'' is $True$ whenever there is at least one $x$ that makes the expression $True$, we know that
$P(5, 0)$ is $True$, or $P(5, 1)$ is $True$, or $P(5, 1)$ is $True$, or \dots, for all possible values of $x$. But
we do not know which one of 0, 1, \dots, is the one that works. So we can't use it directly to rewrite any known
expression involving $P(5, \dots)$. Instead, what we can do is \emph{name} the special $x$ that makes $P(5, x)$
equal to $True$. We often use the convention $C_x$ to name this constant, so we treat $(\exists x.(P(5, x)))$ as
equivalent to $P(5, C_x)$ where $C_x$ is a \emph{new} constant symbol. It's very important that $C_x$ is new, because
we have no way of knowing which is the special value that makes $P(5, \dots)$ $True$. By using a new constant
symbol $C_x$, we are guaranteed that we are not making any other assumptions about its true identity. In most
mathematical proofs, it is common to use a subscripted variable, such as $x_0$, instead of $C_x$ for the name
of the new constant.

So what about proofs? How can we prove a statement that users quantifiers? Our approach is to systematically
remove the quantifiers, so that we are left with a simple statement that does not use them. In other words, we
are left with a simple statement that has only constant symbols, such as $X(2, 3, 1)$. Since there are no variables,
this is really just a Boolean formula, so it can be proved using precisely the same techniques we used for Boolean 
formulas earlier. We do this with a four-step process:
\begin{enumerate}
    \item Rename quantifiers
    \item Migrate quantifiers
    \item Eliminate quantifiers
    \item Prove Boolean formula
\end{enumerate}
The last step is exactly the same as for Boolean formulas, so we only to discuss the first three steps.

Renaming quantifiers is extremely important to prevent a quantifier from accidentally referring to a different variable
that happens to have the same name, much like two different Johns in the same class. For example, consider the formula 
$(\forall x.odd(x)) \vee (\forall x.even(x))$. This
statement is clearly $False$, but only because we understand that the two $x$ variables are different from each other. If
we somehow, e.g., when migrating quantifiers, turned this formula into $(\forall x.(odd(x) \vee even(x)))$ we would be
making a terrible mistake, since this second formula is actually $True$ (at least for the integers). We avoid this possible
error by renaming variables at the beginning of the proof, so we are actually dealing with $(\forall x.odd(x)) \vee (\forall y.even(y))$.

So suppose we have a formula that contains a quantifier $(\forall x.(P(x, \dots)))$. How do we rename the variable $x$ in 
this quantifier? First of all, we insist that when we rename $x$ we use a completely new variable name, that is, one that does
not already appear in the formula. This is for the same reason as before, namely that we do not want to confuse the variable
$x$ with a different variable that happens to have the same name. It would be useless to give $x$ the same name as yet another
variable! Second, we must ensure that when we rename $x$ into, say, $y$, that we replace all occurrences of $x$ with $y$, but
only those occurrences that refer to the \emph{same} $x$. For example, when we rename $x$ in 
$(\forall x.odd(x)) \vee (\forall x.even(x))$, we end up with either
$(\forall y.odd(y)) \vee (\forall x.even(x))$ or
$(\forall x.odd(x)) \vee (\forall y.even(y))$, but \emph{never}
$(\forall y.odd(y)) \vee (\forall y.even(y))$.

For example, consider the theorem 
$$(\exists y. (\forall x. (P(x, y)))) \rightarrow (\forall x. (\exists y. (P(x, y)))),$$
where $P$ is an arbitrary predicate. To prove this theorem, we would first rename the variable $y$ resulting in
$$(\exists v. (\forall x. (P(x, v)))) \rightarrow (\forall x. (\exists y. (P(x, y)))).$$
Then, we would rename the $x$ resulting in
$$(\exists v. (\forall u. (P(u, v)))) \rightarrow (\forall x. (\exists y. (P(x, y)))).$$
Now we have four variables in the formula---$u$, $v$, $x$, and $y$---with a different name for each quantifier, and no
possibility for confusion.

The next step is to migrate the quantifiers at the front. In other words, we want to end up with a formula that looks like
$(\forall x.(\exists y.(\forall z.(\dots))))$, where there are no quantifiers inside the $(\dots)$. We do this by
repeatedly applying the following rules:
\begin{figure}
\begin{center}
\begin{tabular}{ll}
$((\forall x.(P(x, \dots))) \wedge Q) = (\forall x.(P(x, \dots) \wedge Q))$             & \{$\forall\wedge$\} \\
$((\exists x.(P(x, \dots))) \wedge Q) = (\exists x.(P(x, \dots) \wedge Q))$             & \{$\exists\wedge$\} \\
$((\forall x.(P(x, \dots))) \vee Q) = (\forall x.(P(x, \dots) \vee Q))$                 & \{$\forall\vee$\} \\
$((\exists x.(P(x, \dots))) \vee Q) = (\exists x.(P(x, \dots) \vee Q))$                 & \{$\exists\vee$\} \\
$(\neg (\forall x.(P(x, \dots)))) = (\exists x.(\neg (P(x, \dots))))$                   & \{$\neg\forall$\} \\
$(\neg (\exists x.(P(x, \dots)))) = (\forall x.(\neg (P(x, \dots))))$                   & \{$\neg\exists$\} \\
$((\forall x.(P(x, \dots))) \rightarrow Q) = (\exists x.(P(x, \dots) \rightarrow Q))$   & \{$\forall\rightarrow$\} \\
$((\exists x.(P(x, \dots))) \rightarrow Q) = (\forall x.(P(x, \dots) \rightarrow Q))$   & \{$\exists\rightarrow$\} \\
$(Q \rightarrow (\forall x.(P(x, \dots)))) = (\forall x.(Q \rightarrow P(x, \dots)))$   & \{$\rightarrow\forall$\} \\
$(Q \rightarrow (\exists x.(P(x, \dots)))) = (\exists x.(Q \rightarrow P(x, \dots)))$   & \{$\rightarrow\exists$\} \\
\end{tabular}
\end{center}
\caption{Equations of Quantifier Reasoning}
\label{fig-02-quantifiers}
\end{figure}
The first four equations in Figure~\ref{fig-02-quantifiers} are true precisely because the right-hand
side of those formulas, which we denote in the figure with the arbitrary predicate $Q$, is guaranteed
not to refer to the variable $x$ at all, because we have already renamed quantifiers. For example,
suppose that $((\exists x.(P(x, \dots))) \wedge Q)$ is $True$. Then $Q$ must be $True$ and there is 
some $x$, let's say $x_0$, such that $P(x_0, \dots)$ is $True$. But then, $(P(x_0,\dots) \wedge Q)$
is $True$, so $(\exists x.(P(x, \dots) \wedge Q))$ is also $True$. The important point is that the
choice of $x$ cannot possibly affect the truth value of $Q$, so moving the quantifier above the
$\vee$ or $\wedge$ does not make a difference.

The rules $\{\neg\forall\}$ and $\{\neg\exists\}$ may appear more mysterious at first, but they actually
capture the everyday meaning of ``for all'' and ``there exists.'' For example, suppose that it is not
the case that $P(x,\dots)$ is $True$ for all values of $x$. That is just the same as saying that there
is some value of $x$ such that $P(x,\dots)$ is not $True$, and that is precisely what 
\{$\neg\forall$\} says more formally.

Finally, the last four equations in Figure~\ref{fig-02-quantifiers} describe how quantifiers behave
inside implications. When the quantifier is in the left-hand side of an implication, it flips from
``for all'' to ``there exists'' and vice versa when it is migrated through the implication, but it remains the same when it appears in the right-hand side. This may seem surprising, and many students
find it completely unintuitive at first, but remember that $p \rightarrow q$ is the same as
$(\neg p) \vee q$ by \{implication\}, so if the quantifier appears in the left-hand side of
the implication, it is essentially the $\neg$ that flips it before it can migrate above the
implication. In other words, we had no choice in these four equations, since they can be derived
from the others, in particular from \{implication\}, \{$\neg\forall$\}, and \{$\neg\exists$\}.

Let's consider again the theorem 
$$(\exists y.(\forall x.(P(x, y)))) \rightarrow (\forall x.(\exists y (P(x, y)))),$$
which we have already converted to
$$(\exists v.(\forall u.(P(u, v)))) \rightarrow (\forall x.(\exists y.(P(x, y)))),$$
by renaming the first $x$ and $y$ variables. At this point, we have a choice of variables that
we can migrate to the front, either $v$ or $x$. We will consider the more advantageous choice
later, but for now we will simply state that migrating $x$ first is a good idea, resulting in
$$(\forall x.((\exists v.(\forall u.(P(u, v)))) \rightarrow (\exists y.(P(x, y))))).$$
Again we have a choice of which variable to migrate, and this time we will choose $v$, resulting in
$$(\forall x.(\forall v.((\forall u.(P(u, v))) \rightarrow (\exists y.(P(x, y)))))).$$
To continue migrating quantifiers, we can choose either $v$ or $y$ at this stage. As it happens,
either choice will work, so we choose to migrate $u$ first, resulting in
$$(\forall x.(\forall v.(\exists u.(P(u, v) \rightarrow (\exists y.(P(x, y))))))),$$
and finally
$$(\forall x.(\forall v.(\exists u.(\exists y.(P(u, v) \rightarrow P(x, y)))))).$$

Once the quantifiers are in the front, we remove them to result in an expression that has
only constant symbols. Removing a ``for all'' quantifier is easy. Suppose we wish to show
that $(\forall x.(P(x, \dots)))$ is $True$. Since $P(x, \dots)$ must be true for all possible
values of $x$, we can prove this by replacing $x$ with a \emph{completely arbitrary} constant.
That is, if we can show that $P(\text{fred}, \dots)$ is true without making any assumptions
at all on ``fred'', then we can safely say that $P(x, \dots)$ is true for all values of $x$.
So to prove $(\forall x.(P(x, \dots)))$ we simply replace the variable $x$ with an
\emph{arbitrary} constant $x_0$, and prove $P(x_0, \dots)$. It is vital that $x_0$ is a \emph{new}
constant symbol, so make sure that it does not accidentally appear anywhere else in $P$.

For example, in the proof of 
$$(\exists y.(\forall x.(P(x, y)))) \rightarrow (\forall x.(\exists y (P(x, y)))),$$
we rewrote the expression as
$$(\forall x.(\forall v.(\exists u.(\exists y.(P(u, v) \rightarrow P(x, y))))))$$
by renaming the quantifiers and migrating them to the front. The proof can now proceed
by removing the ``for all'' quantifier for $x$, resulting in
$$(\forall v.(\exists u.(\exists y.(P(u, v) \rightarrow P(x_0, y)))))).$$
Notice that the constant symbol $x_0$ is completely new, in that it does not appear anywhere
else in the formula. Next, we can remove the quantifier for $v$, resulting in
$$(\exists u.(\exists y.(P(u, v_0) \rightarrow P(x_0, y))))).$$

That brings us up to removing the ``exists'' quantifiers. This is actually a very delicate
stage in the proof! Suppose we are trying to prove that
$$(\exists x.(x > 10)).$$
We do this by \emph{demonstrating} that $x>10$ for a suitable value of $x$, and we can choose
the value of $x$ to use. For example, we
can replace $x$ with 20, resulting in $20 > 10.$ This is, of course, $True$ so we have
proved that $(\exists x.(x > 10)),$ as desired. But what if we replace $x$ with 5? Then we
would end up with $5 > 10,$ which is clearly $False$. What do we make of this? One thing
we cannot conclude is that $(\exists x.(x > 10))$ is $False$! We just made a bad choice of $x$,
but that does not mean that some other choice of $x$ (e.g., 20) wouldn't have worked. So
the good news is that we can choose the value of $x$, but the bad news is we have to make
a wise decision!

Returning to our running example, let's finish the proof of 
$$(\exists y.(\forall x.(P(x, y)))) \rightarrow (\forall x.(\exists y (P(x, y)))).$$
By renaming and migrating quantifiers, and removing the ``for all'' quantifiers, we reduced
this to
$$(\exists u.(\exists y.(P(u, v_0) \rightarrow P(x_0, y))))).$$
Now we need to remove the remaining ``there exists'' quantifiers, and we can choose any
expression to use in place of the variables $u$ and $y$. Our job then is to choose a value
of $u$ that will let us prove the resulting expression. For example, let's say we choose
to replace $u$ with $0$ and $y$ with $5$, resulting in
$$P(0, v_0) \rightarrow P(x_0, 5).$$
At this point, we'd be stuck, just as when we ended up with $5 > 10$. The problem is that
there's simply no reason why $P(0, v_0)$ should imply $P(x_0, 5)$. But we can make a much
better choice. For instance, let's suppose that we replace $u$ with $x_0$:
$$(\exists y.(P(x_0, v_0) \rightarrow P(x_0, y)))).$$
This looks much promising, doesn't it? Then we can replace $y$ with $v_0$:
$$P(x_0, v_0) \rightarrow P(x_0, v_0).$$
At this point, we have a formula with \emph{no} variables. In particular, remember that
$x_0$ and $v_0$ are \emph{constant} symbols. If $P$ is a predicate about the integers, we 
may not know what integer $x_0$ is equal to, but we can be sure that $x_0$ is an integer.
Now that we have a variable-free formula, we are essentially back to propositional reasoning,
and we can finish the proof using the axioms in Figure~\ref{fig-02-03}. In this particular
case, we can end by invoking \{self-implication\}.

That also explains why we chose to migrate $x$, then $v$, and finally $u$ and $y$.
We were motivated by the desire to wait as long as possible before having to make
a choice. That means that whenever we have a choice, we want to migrate a quantifier
that will become a ``for all'' before we migrate one that will become a ``there exists.'' 
For example, when we were migrating quantifiers on the following formula, we had to 
decide whether to migrate the variable $v$ or $x$:
$$(\exists v.(\forall u.(P(u, v)))) \rightarrow (\forall x.(\exists y.(P(x, y)))).$$
Migrating the variable $x$ would result in a ``for all '' quantifier in the front,
as would migrating the $v$. So there's no benefit to either one, and we can choose
essentially at random. We chose to migrate the $x$, but let's decide to migrate
the $v$ this time:
$$(\forall v.((\forall u.(P(u, v))) \rightarrow (\forall x.(\exists y.(P(x, y)))))).$$
Now, there is a choice of migrating the $u$ or $x$, but this time it \emph{does}
matter. If we migrate the $x$ first, we move a ``for all'' quantifier to the front,
but if we migrate the $u$ we end up with ``there exists.'' We want to delay decisions
as long as possible, so it is to our benefit to migrate the $x$ quantifier first:
$$(\forall v.((\forall x.((\forall u.(P(u, v))) \rightarrow (\exists y.(P(x, y))))))).$$

It seems intuitive that delaying having to make a choice as much as possible is
beneficial, but in fact it's even more than that. It's actually necessary. For example,
take the following statement, which is $True$ when referring to the integers:
$$(\forall x.((\exists y.(x < y)))).$$
To prove this theorem, we would first remove the ``for all'' quantifier to end up with
$$(\exists y.(x_0 < y)).$$
Now it is easy to find a suitable value of $y$, namely $x_0+1$, ending up with
$$x_0 < x_0 + 1.$$

On the other hand, suppose we were trying to prove the following statement, which
is \emph{actually false}:
$$(\exists y.((\forall x.(x < y)))).$$
We are forced to remove the quantifier for the variable $y$, but that means we have
to choose to correct value of $y$, and we cannot possibly know that until we know
the value of $x$.
Now, you may think that we can replace $y$ with the expression $x+1$, but this would
not be allowed. The reason is that $x+1$ is not a constant expression, and we can
only replace a variable with a constant. And in case you think this is an arbitrary
rule, remember that the statement $(\exists y.((\forall x.(x < y))))$ is \emph{not}
$True$. It is not true that there exists a \emph{single} value of $y$ that is larger
than all possible values of $x$.

So in general, the order in which quantifiers are migrated to the front \emph{is} 
important. And that's why we always migrate the quantifiers so that, as far as
possible, ``for all'' quantifiers appear before ``there exists'' quantifiers.

We end this section with a disclaimer. The methods described here offer one way of
proving statements that use quantifiers, but not the only way of doing so. For
instance, it is possible to extend the techniques of natural deduction to include
quantifiers. Ultimately, which technique you use is a matter of preference, but
we like the methods introduced here since they are similar to the way the way the
automated theorem prover ACL2 works, as we will see later in the following chapters.

\begin{ExerciseList}
\Exercise Prove that $(\forall x.(P(x))) \rightarrow (\exists x.(P(x)))$.
\Exercise What goes wrong when you attempt to prove that $(\exists x.(P(x))) \rightarrow (\forall x.(P(x)))$? Be sure to explain
    why this is different than the previous exercise.
\Exercise Prove that $((\forall x.(P(x) \rightarrow Q(x))) \wedge (\forall x.(Q(x) \rightarrow R(x)))) \rightarrow (\forall x.(P(x) \rightarrow R(x)))$.
\end{ExerciseList}
